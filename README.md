# üß† Machine Learning: An In-Depth Guide

Welcome! This repository consists of detailed **Jupyter Notebooks** designed to break down and explain the core concepts of Machine Learning. 

If you are looking to understand the math, logic, and implementation of specific ML algorithms, check out the notebooks uploaded in this repository.

## üìö What's Inside?

I have written in-depth explanations for the following modules. Click on the relevant notebook files above to start learning.

---

### 1. Supervised Learning
*Algorithms that learn from labeled training data.*

#### üìà Regression Models
*Used for predicting continuous values (e.g., salary, price, temperature).*
* **Simple Linear Regression:** The foundation of fitting a line to data.
* **Multiple Linear Regression:** Handling multiple input variables.
* **Polynomial Regression:** Modeling non-linear relationships.
* **Support Vector Regression (SVR):** Using support vectors for continuous prediction.
* **Decision Tree Regression:** Splitting data into branches to predict values.
* **Random Forest Regression:** Ensemble learning using multiple decision trees.

#### üéØ Classification Models
*Used for predicting categories (e.g., spam vs. not spam, cat vs. dog).*
* **Logistic Regression:** The classic binary classification algorithm.
* **K-Nearest Neighbors (K-NN):** Classification based on feature similarity.
* **Support Vector Machines (SVM):** Finding the optimal hyperplane to separate classes.
* **Kernel SVM:** Handling non-linear data using kernel functions.
* **Naive Bayes:** Probabilistic classification based on Bayes' Theorem.
* **Decision Tree Classification:** Visual and intuitive classification rules.
* **Random Forest Classification:** Robust classification using ensemble trees.

---

### 2. Unsupervised Learning
*Algorithms that find hidden patterns in unlabeled data.*

#### üß© Clustering
*Grouping similar data points together.*
* **K-Means Clustering:** Iterative partitioning of data into K groups.
* **Hierarchical Clustering:** Building a hierarchy of clusters (Dendrograms).

#### üìâ Dimensionality Reduction
*Reducing the number of variables while keeping the important information.*
* **Principal Component Analysis (PCA):** Linear dimensionality reduction.
* **Linear Discriminant Analysis (LDA):** Supervised dimensionality reduction for class separation.
* **Kernel PCA:** Non-linear dimensionality reduction.

---

### 3. Advanced Ensemble Models
*High-performance algorithms often used in competitions.* *(Note: These notebooks provide a practical overview and implementation guide, rather than a deep mathematical deep-dive like the sections above.)*

* **XGBoost:** Extreme Gradient Boosting for speed and performance.
* **CatBoost:** Gradient boosting optimized for categorical data.
* **LightGBM:** A fast, distributed, high-performance gradient boosting framework.

---

## üõ†Ô∏è How to Use
1.  Clone this repository or download the files.
2.  Open the `.ipynb` files using **Jupyter Notebook**, **JupyterLab**, or **Google Colab**.
3.  Read through the markdown cells for the theory and run the code cells to see the models in action.

## ü§ù Contributing
If you notice any errors or have suggestions for more models to add, feel free to open a pull request!
